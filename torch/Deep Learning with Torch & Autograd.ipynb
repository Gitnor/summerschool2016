{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Torch and Torch Autograd\n",
    "\n",
    "(Much thanks to Soumith Chintala for providing intro material)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of this talk\n",
    "* Understand torch and the neural networks package at a high-level.\n",
    "* Train a small neural network on CPU and GPU\n",
    "* Try out using autograd with your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Torch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch is an scientific computing framework based on Lua[JIT] with strong CPU and CUDA backends.\n",
    "\n",
    "Strong points of Torch:\n",
    "\n",
    "* Efficient Tensor library (like NumPy) with an efficient CUDA backend\n",
    "* Neural Networks package -- build arbitrary acyclic computation graphs with automatic differentiation\n",
    "   * also with fast CUDA and CPU backends\n",
    "* Good community and industry support - several hundred community-built and maintained packages.\n",
    "* Easy to use Multi-GPU support and parallelizing neural networks\n",
    "\n",
    "http://torch.ch  \n",
    "https://github.com/torch/torch7/wiki/Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Torch Autograd?\n",
    "\n",
    "https://github.com/twitter/torch-autograd\n",
    "\n",
    "Torch autograd is an automatic differentiation package, that will automatically calculate derivatives of any differentiable Torch function for you. For some function \n",
    "```\n",
    "function f(params,...)\n",
    "out = f(..)\n",
    "``` \n",
    "with input `params` that is either a table of tensors, or a tensor, just call \n",
    "```\n",
    "g = grad(f)\n",
    "gradients, out = g(...)\n",
    "```\n",
    "to get a new function `g` that will return the output of `f`, along with the gradients of `f w.r.t. params`.\n",
    "\n",
    "Where `NN` and related packages are mostly restricted to backpropagating through static compute graphs, autograd has no such limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on Lua and runs on Lua-JIT (Just-in-time compiler) which is fast\n",
    "* Lua is pretty close to javascript.\n",
    "   * variables are global by default, unless `local` keyword is used\n",
    "* Only has one data structure built-in, a table: `{}`. Doubles as a hash-table and an array.\n",
    "* 1-based indexing.\n",
    "* `foo:bar()` is the same as `foo.bar(foo)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "#### Strings, numbers, tables - a tiny introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[1] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[2] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1,#b do -- the # operator is the length operator in Lua\n",
    "    print(b[i]) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.Tensor(5,3) -- construct a 5x3 matrix, uninitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.rand(5,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=torch.rand(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 1\n",
    "a*b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 2\n",
    "torch.mm(a,b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- matrix-matrix multiplication: syntax 3\n",
    "c=torch.Tensor(5,4)\n",
    "c:mm(a,b) -- store the result of a*b in c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Tensors\n",
    "Tensors can be moved onto GPU using the :cuda function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "require 'cutorch';\n",
    "a = a:cuda()\n",
    "b = b:cuda()\n",
    "c = c:cuda()\n",
    "c:mm(a,b) -- done on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Add two tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function addTensors(a,b)\n",
    "    return a -- FIX ME\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.ones(5,2)\n",
    "b = torch.Tensor(3,4):fill(4)\n",
    "print(addTensors(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "Neural networks in Torch can be constructed using the `nn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Modules` are the bricks used to build neural networks. Each are themselves neural networks, but can be combined with other networks using `containers` to create complex neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, look at this network that classfies digit images:\n",
    "![LeNet](http://fastml.com/images/cifar/lenet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a simple feed-forward network.   \n",
    "It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
    "\n",
    "Such a network container is `nn.Sequential` which feeds the input through several layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net:add(nn.SpatialConvolution(1, 6, 5, 5)) -- 1 input image channel, 6 output channels, 5x5 convolution kernel\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))     -- A max-pooling operation that looks at 2x2 windows and finds the max.\n",
    "net:add(nn.SpatialConvolution(6, 16, 5, 5))\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "net:add(nn.View(16*5*5))                    -- reshapes from a 3D tensor of 16x5x5 into 1D tensor of 16*5*5\n",
    "net:add(nn.Linear(16*5*5, 120))             -- fully connected layer (matrix multiplication between input and weights)\n",
    "net:add(nn.Linear(120, 84))\n",
    "net:add(nn.Linear(84, 10))                   -- 10 is the number of outputs of the network (in this case, 10 digits)\n",
    "net:add(nn.LogSoftMax())                     -- converts the output to a log-probability. Useful for classification problems\n",
    "\n",
    "print('Lenet5\\n' .. net:__tostring());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other examples of nn containers are shown in the figure below:\n",
    "![containers](https://raw.githubusercontent.com/soumith/ex/gh-pages/assets/nn_containers.png)\n",
    "\n",
    "Every neural network module in torch has its forward computation defined, as well as its partial derivatives.\n",
    "It has a `:forward(input)` function that computes the output for a given input, flowing the input through the network.\n",
    "and it has a `:backward(input, gradient)` function that will differentiate each neuron in the network w.r.t. the gradient that is passed in. This is done via the chain rule. Calling `forward` and `backward` on the whole model (e.g. the `nn.Sequential()` container) will compose all of the functions together to calculate `forward`, and calling `backward` will calculate the gradients of loss with respect to the inputs through all of the modules, using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = torch.rand(1,32,32) -- pass a random tensor as input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = net:forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net:zeroGradParameters() -- zero the internal gradient buffers of the network (will come to this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradInput = net:backward(input, torch.rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(#gradInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion: Defining a loss function\n",
    "When you want a model to learn to do something, you give it feedback on how well it is doing. This function that computes an objective measure of the model's performance is called a __loss function__.\n",
    "\n",
    "A typical loss function takes in the model's output and the groundtruth and computes a value that quantifies the model's performance.\n",
    "\n",
    "The model then corrects itself to have a smaller loss.\n",
    "\n",
    "In torch, loss functions are implemented just like neural network modules, and have automatic differentiation.  \n",
    "They have two functions - `forward(input, target)`, `backward(input, target)`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion() -- a negative log-likelihood criterion for multi-class classification\n",
    "criterion:forward(output, 3) -- let's say the groundtruth was class number: 3\n",
    "gradients = criterion:backward(output, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradInput = net:backward(input, gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Review of what you learnt so far\n",
    "* Network can have many layers of computation\n",
    "* Network takes an input and produces an output in the :forward pass\n",
    "* Criterion computes the loss of the network, and it's gradients w.r.t. the output of the network.\n",
    "* Network takes an (input, gradients) pair in it's backward pass and calculates the gradients w.r.t. each layer (and neuron) in the network.\n",
    "\n",
    "##### Missing details\n",
    "> A neural network layer can have learnable parameters or not.\n",
    "\n",
    "A convolution layer learns it's convolution kernels to adapt to the input data and the problem being solved.  \n",
    "A max-pooling layer has no learnable parameters. It only finds the max of local windows.\n",
    "\n",
    "A layer in torch which has learnable weights, will typically have fields .weight (and optionally, .bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = nn.SpatialConvolution(1,3,2,2) -- learn 3 2x2 kernels\n",
    "print(m.weight) -- initially, the weights are randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(m.bias) -- The operation in a convolution layer is: output = convolution(input,weight) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also two other important fields in a learnable layer. The gradWeight and gradBias.\n",
    "The gradWeight accumulates the gradients w.r.t. each weight in the layer, and the gradBias, w.r.t. each bias in the layer.\n",
    "\n",
    "#### Training the network\n",
    "\n",
    "For the network to adjust itself, it typically does this operation (if you do Stochastic Gradient Descent):\n",
    "> weight = weight + learningRate * gradWeight [equation 1]\n",
    "\n",
    "This update over time will adjust the network weights such that the output loss is decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it is time to discuss one missing piece. Who visits each layer in your neural network and updates the weight according to Equation 1?\n",
    "\n",
    "There are multiple answers, but we will use the answer that gives us the most flexibility.\n",
    "We'll use `autograd` to join together the network and the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about data?\n",
    "Generally, when you have to deal with image, text, audio or video data, you can use standard functions like: [__image.load__](https://github.com/torch/image#res-imageloadfilename-depth-tensortype) or [__audio.load__](https://github.com/soumith/lua---audio#usage) to load your data into a _torch.Tensor_ or a Lua table, as convenient.\n",
    "\n",
    "Let us now use some simple data to train our network.\n",
    "\n",
    "We shall use the CIFAR-10 dataset, which has the classes: 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'.  \n",
    "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "![CIFAR-10 image](https://raw.githubusercontent.com/soumith/ex/gh-pages/assets/cifar10.png)\n",
    "\n",
    "The dataset has 50,000 training images and 10,000 test images in total.\n",
    "\n",
    "__We now have 5 steps left to do in training our first torch neural network__\n",
    "1. Load and normalize data\n",
    "2. Define Neural Network\n",
    "3. Define Loss function\n",
    "4. Train network on training data\n",
    "5. Test network on test data.\n",
    "\n",
    "__1. Load and normalize data__\n",
    "\n",
    "Today, in the interest of time, we prepared the data before-hand into a 4D torch ByteTensor of size 10000x3x32x32 (training) and 10000x3x32x32 (testing)\n",
    "Let us load the data and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.execute('wget -c https://s3.amazonaws.com/torch7/data/cifar10torchsmall.zip')\n",
    "os.execute('unzip cifar10torchsmall.zip')\n",
    "trainset = torch.load('cifar10-train.t7')\n",
    "-- First, let's make sure our data is the right type\n",
    "trainset.data = trainset.data:double() -- convert the data from a ByteTensor to a DoubleTensor.\n",
    "testset = torch.load('cifar10-test.t7')\n",
    "testset.data = testset.data:double() -- convert the data from a ByteTensor to a DoubleTensor.\n",
    "classes = {'airplane', 'automobile', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(#trainset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let us display an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itorch.image(trainset.data[100]) -- display the 100-th image in dataset\n",
    "print(classes[trainset.label[100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a function to grab a batch from our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 8\n",
    "function makeBatch(dataset,didx)\n",
    "   didx = (didx-1) * batchSize + 1\n",
    "   return {\n",
    "      data = dataset.data[{ {didx,didx+batchSize-1} }],\n",
    "      label = dataset.label[{ {didx,didx+batchSize-1} }],\n",
    "   }\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- TODO:\n",
    "-- View all the images in the batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Solution\n",
    "batch = makeBatch(trainset,1)\n",
    "for i=1,batch.data:size(1) do\n",
    "    itorch.image(batch.data[i])\n",
    "    print(classes[batch.label[i]])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__One of the most important things you can do in conditioning your data (in general in data-science or machine learning) is to make your data to have a mean of 0.0 and standard-deviation of 1.0.__\n",
    "\n",
    "Let us do that as a final step of our data processing.\n",
    "\n",
    "To do this, we introduce you to the tensor indexing operator.\n",
    "It is shown by example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redChannel = trainset.data[{ {}, {1}, {}, {}  }] -- this picks {all images, 1st channel, all vertical pixels, all horizontal pixels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Each channel individually will be viewable as greyscale\n",
    "for i=1,3 do\n",
    "    itorch.image(trainset.data[{ {}, {i}, {}, {}  }][1]) -- a single channel will show as a black and white image\n",
    "end\n",
    "itorch.image(trainset.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this indexing operator, you initally start with ___[{ }]___. You can pick all elements in a dimension using ___{}___ or pick a particular element using ___{i}___ where ___i___ is the element index. You can also pick a range of elements using ___{i1, i2}___, for example ___{3,5}___ gives us the 3,4,5 elements.\n",
    "\n",
    "__Exercise: Select the 150th to 300th data elements of the data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving back to mean-subtraction and standard-deviation based scaling, doing this operation is simple, using the indexing operator that we learnt above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = {} -- store the mean, to normalize the test set in the future\n",
    "stdv  = {} -- store the standard-deviation for the future\n",
    "for i=1,3 do -- over each image channel\n",
    "    mean[i] = trainset.data[{ {}, {i}, {}, {}  }]:mean() -- mean estimation\n",
    "    print('Channel ' .. i .. ', Mean: ' .. mean[i])\n",
    "    trainset.data[{ {}, {i}, {}, {}  }]:add(-mean[i]) -- mean subtraction\n",
    "    \n",
    "    stdv[i] = trainset.data[{ {}, {i}, {}, {}  }]:std() -- std estimation\n",
    "    print('Channel ' .. i .. ', Standard Deviation: ' .. stdv[i])\n",
    "    trainset.data[{ {}, {i}, {}, {}  }]:div(stdv[i]) -- std scaling\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the cell above twice... you'll see the mean shifts to close to 0,\n",
    "and the standard deviation becomes close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our network to operate on RGB (3-channel) input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Copy the neural network from the __Neural Networks__ section above and modify it to take 3-channel images (instead of 1-channel images as it was defined).  \n",
    "Hint: You only have to change the first layer, change the number 1 to be 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the solution, just need to alter the number of inputs channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net:add(nn.SpatialConvolution(3, 6, 5, 5)) -- 3 input image channel, 6 output channels, 5x5 convolution kernel\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))     -- A max-pooling operation that looks at 2x2 windows and finds the max.\n",
    "net:add(nn.SpatialConvolution(6, 16, 5, 5))\n",
    "net:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "net:add(nn.View(16*5*5))                    -- reshapes from a 3D tensor of 16x5x5 into 1D tensor of 16*5*5\n",
    "net:add(nn.Linear(16*5*5, 120))             -- fully connected layer (matrix multiplication between input and weights)\n",
    "net:add(nn.Linear(120, 84))\n",
    "net:add(nn.Linear(84, 10))                   -- 10 is the number of outputs of the network (in this case, 10 digits)\n",
    "net:add(nn.LogSoftMax())                     -- converts the output to a log-probability. Useful for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase our network as a predictor and a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Wrap our neural network in a function we can use\n",
    "-- to make a prediction, given a model\n",
    "\n",
    "params, gradParams = net:getParameters()\n",
    "function predict(model, input)\n",
    "   model:forward(input)\n",
    "   return model.output\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Now, wrap our prediction function, along with\n",
    "-- our criterion, so that we can get the gradients\n",
    "-- of the loss with respect to the parameters\n",
    "\n",
    "function df(model, criterion, input, target)\n",
    "   -- First, zero out the gradient parameters\n",
    "   gradParams:zero()\n",
    "   -- Make a prediction\n",
    "   prediction = predict(model, input)\n",
    "   -- Now, check how good our prediction was, through our criterion\n",
    "   loss = criterion:forward(prediction, target)\n",
    "   -- Now, propagate the error as a gradient through the criterion\n",
    "   -- and the model\n",
    "   model:backward(input, criterion:backward(prediction, target))\n",
    "   -- Return the loss, the gradient, and the original prediction\n",
    "   return loss, gradParams, prediction\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a loss function as well. Log-likelihood classification loss works well for a lot of classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that we can learn to predict the label of just a single image\n",
    "A good debugging tip -- if your neural network can't overfit your data, something is wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Make sure we've imported plotting code\n",
    "Plot = require 'itorch.Plot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Notice that before we train, the probabilities of a given class are all mostly uniform\n",
    "classLogProbabilities = predict(net, trainset.data[1])\n",
    "classProbabilities = classLogProbabilities:exp() -- we get out log probabilities, but we'd like regular ones\n",
    "p = Plot()\n",
    "    :line(torch.range(1,#classes),classProbabilities)\n",
    "    :yaxis(\"Class probabilities (0-1)\")\n",
    "    :xaxis(\"Class ID\")\n",
    "    :title(\"Class probabilities of a single sample (untrained)\")\n",
    "    :draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running this cell below a few times, you'll see your net begin to overfit on the single training sample we've asked it to learn on. If it's not doing that, something weird is going on!  \n",
    "  \n",
    "When you're convinced yourself we're able to overfit, go back and reset the network by running the cells that a) define the network (cell starts with `net = `), defines the `predict` function and defines the `df` function. Then, skip the cell below, so you don't accidentally overfit to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- SGD on one image + label\n",
    "loss, gradParams, prediction = df(net, criterion, trainset.data[1], trainset.label[1])\n",
    "\n",
    "-- Now update the parameters with some learning rate\n",
    "params:add(-1e-2, gradParams)\n",
    "\n",
    "-- And now make a plot to see what our class probabilities look like\n",
    "classLogProbabilities = predict(net, trainset.data[1])\n",
    "-- print(prediction,trainset.label[1])\n",
    "classProbabilities = torch.exp(classLogProbabilities) -- we get out log probabilities, but we'd like regular ones\n",
    "p = Plot()\n",
    "    :line(torch.range(1,#classes),classProbabilities)\n",
    "    :yaxis(\"Class probabilities (0-1)\")\n",
    "    :xaxis(\"Class ID\")\n",
    "    :title(\"Class probabilities of a single sample (untrained)\")\n",
    "    :draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, we're ready to loop over our data and really learn. First -- some reporting\n",
    "\n",
    "We've convinced ourselves the network can learn something, at least it can overfit on a single sample.  \n",
    "However, can it learn to make accurate predictions across a distribution of input data, and for more than just one input class?  \n",
    "In order to track performance during training, we'll use a confusion matrix to visualize how often the network is making correct predictions, and what are the types of mistakes it is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- The library 'optim' contains lots of useful utilities, but we'll just use this one guy\n",
    "confusionMatrix = require('optim').ConfusionMatrix(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our reporting is ready. Let's learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Loop over the data for a few epochs, with xlua reporting, confusion matrix accumulation.\n",
    "nEpochs = 5\n",
    "nTrainingExamples = trainset.data:size(1)\n",
    "learningRate = 1e-3\n",
    "\n",
    "for e=1,nEpochs do\n",
    "   confusionMatrix:zero()\n",
    "   print(\"Working on epoch \"..tostring(e)..\"/\"..tostring(nEpochs))\n",
    "\n",
    "   -- Training loop\n",
    "   for i=1,nTrainingExamples/batchSize do\n",
    "\n",
    "      -- Get data\n",
    "      data = makeBatch(trainset, i)\n",
    "\n",
    "      -- Get gradient and prediction\n",
    "      loss, gradients, prediction = df(net, criterion, data.data, data.label)\n",
    "\n",
    "      -- Update parameters\n",
    "      params:add(-learningRate, gradients)\n",
    "\n",
    "      -- Update confusion matrix\n",
    "      for i=1,prediction:size(1) do\n",
    "         confusionMatrix:add(prediction[i], data.label[i])\n",
    "      end\n",
    "\n",
    "   end\n",
    "   print(confusionMatrix)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're reducing training error. But how are we doing on a validation dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test the network, print accuracy__\n",
    "\n",
    "We have trained the network over the training dataset. It's doing ok, not great.  \n",
    "But we need to check if the network has overfit, or can generalize to similar data.  \n",
    "We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to normalize the validation dataset, using the same statistics we computed for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset.data = testset.data:double()   -- convert from Byte tensor to Double tensor\n",
    "for i=1,3 do -- over each image channel\n",
    "    testset.data[{ {}, {i}, {}, {}  }]:add(-mean[i]) -- mean subtraction    \n",
    "    testset.data[{ {}, {i}, {}, {}  }]:div(stdv[i]) -- std scaling\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, first step. Let us display an image from the test set to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classes[testset.label[100]])\n",
    "itorch.image(testset.data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- for fun, print the mean and standard-deviation of example-100\n",
    "horse = testset.data[100]\n",
    "print(horse:mean(), horse:std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classes[testset.label[100]])\n",
    "itorch.image(testset.data[100])\n",
    "predicted = predict(net, testset.data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- the output of the network is Log-Probabilities. To convert them to probabilities, you have to take e^x \n",
    "predictedProb = torch.exp(predicted)\n",
    "print(predictedProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the network predictions. The network assigned a probability to each classes, given the image.\n",
    "\n",
    "To make it clearer, let us tag each probability with it's class-name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i=1,predicted:size(1) do\n",
    "    print(classes[i], predictedProb[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look so good. On my system, this predicts that the horse is a truck. A horsetruck. But how are we doing on the test set as a whole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "confusionMatrix:zero()\n",
    "for i=1,testset.data:size(1) do\n",
    "    local groundtruth = testset.label[i]\n",
    "    local prediction = predict(net,testset.data[i])\n",
    "    confusionMatrix:add(prediction, testset.label[i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Try getting the network on the GPU__\n",
    "  \n",
    "I'll list the ingredients for getting a network onto the GPU below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, require 'cutorch' and 'cunn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'cutorch'\n",
    "require 'cunn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you just have to cast your network to the GPU type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = net:cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, transfer the criterion to GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = criterion:cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset.data = trainset.data:cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's train on GPU :) #sosimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Copy the training code from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why dont I notice MASSIVE speedup compared to CPU?  \n",
    "Because your network is quite small.  \n",
    "  \n",
    "*Exercise:* Try increasing the size of your network (argument 1 and 2 of nn.SpatialConvolution(...), see what kind of speedup you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Logging training error__  \n",
    "  \n",
    "You can try plotting your training error every epoch to visually monitor learning progress.  \n",
    "The code below updates a graph every 100 milliseconds. You can try adapting it to your training loop to show\n",
    "the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- To get the average class accuracy from a ConfusionMatrix class, you can try\n",
    "-- confusionMatrix:updateValids()\n",
    "-- print(confusionMatrix.averageValid)\n",
    "\n",
    "Plot = require 'itorch.Plot'\n",
    "\n",
    "pts = {}\n",
    "err = {}\n",
    "\n",
    "p = nil\n",
    "for i=1,30 do\n",
    "    pts[#pts+1] = i\n",
    "    err[#err+1] = 1.0/i\n",
    "    if not p then\n",
    "        p = Plot():line(pts,err):title('Training Error'):redraw()\n",
    "    else\n",
    "        p:line(pts,err):redraw()\n",
    "    end\n",
    "    sys.execute(\"sleep 0.1\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use autograd to train the network from above__  \n",
    "  \n",
    "  \n",
    "Autograd lets you write arbitrary torch functions, and automatically get the derivative of those functions. Here's a small example:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Some crazy torch function, with if statements (we could have while and for loops, too)\n",
    "function f(a,b)\n",
    "    c = a * b\n",
    "    if b > 0 then\n",
    "        d = torch.log(c)\n",
    "    else\n",
    "        d = torch.sin(c)\n",
    "    end\n",
    "    return d\n",
    "end\n",
    "print(\"Value: \" .. f(2,3))\n",
    "\n",
    "-- Calling grad() on a function returns a NEW function, that calculates the value as well as the gradient\n",
    "-- of the original function (gradient w.r.t. first argument only, but the argument can be a table of tensors)\n",
    "grad = require 'autograd'\n",
    "g = grad(f)\n",
    "\n",
    "gradient, value = g(2,3)\n",
    "print(\"\\nValue: \"..value)\n",
    "print(\"Gradient:\"..gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In autograd, you have to write your code in a procedural style, and be careful not to overwrite the internals of any variables.\n",
    "So, instead of doing\n",
    "\n",
    "```lua\n",
    "a = torch.randn(3,3)\n",
    "a:exp() -- now the content of a has been exponentiated. doesn't work in autograd, though!\n",
    "```\n",
    "\n",
    "do\n",
    "\n",
    "```lua\n",
    "a = torch.randn(3,3)\n",
    "a = torch.exp(a)\n",
    "```\n",
    "\n",
    "however, this is generally a bad idea in autograd\n",
    "```lua\n",
    "a = torch.randn(3,3)\n",
    "-- can't overwrite indices! because when doing backprop, \n",
    "-- we've lost the original content of a[i]\n",
    "a[1] = torch.exp(a[1]) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using torch functions like `torch.exp`, `torch.mm`, `torch.log` and ops like `+,/,-,*` are no problem in autograd. But there's been an enormous amount of work put into building really efficient `NN` modules. Fortunately, we can use those in autograd as well, but we have to first convert them from objects (which are strung together using containers, as we saw above), into functions. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_cnn, params_cnn = grad.functionalize(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened? We created a function which takes in the neural net parameters,  \n",
    "as well as input data, and calls the underlying `nn.Sequential` model and returns the results. Under the hood, we'll also handle backpropagation if this function is used, and we need gradients through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_cnn(params_cnn, trainset.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can construct the trainable model that we had above as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fCriterion = grad.functionalize(criterion) -- we can functionalize criterions, but they don't have parameters\n",
    "\n",
    "function f(params, input, target)\n",
    "   prediction = predict_cnn(params, input)\n",
    "   loss = fCriterion(prediction, target)\n",
    "   return loss, prediction\n",
    "end\n",
    "\n",
    "-- Autograd will give you a function that calculates gradients\n",
    "df = grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradParams, loss, prediction = df(params_cnn, trainset.data[1], trainset.label[1])\n",
    "\n",
    "-- Now update the parameters with some learning rate\n",
    "for i=1,#params_cnn do -- loop over each parameter (they're no longer one big tensor)\n",
    "    params_cnn[i]:add(-1e-2,gradParams[i]) -- but gradParams is the same size\n",
    "end\n",
    "\n",
    "-- And now make a plot to see what our class probabilities look like\n",
    "classLogProbabilities = predict_cnn(params_cnn, trainset.data[1])\n",
    "classProbabilities = torch.exp(classLogProbabilities) -- we get out log probabilities, but we'd like regular ones\n",
    "p = Plot()\n",
    "    :line(torch.range(1,#classes),classProbabilities)\n",
    "    :yaxis(\"Class probabilities (0-1)\")\n",
    "    :xaxis(\"Class ID\")\n",
    "    :title(\"Class probabilities of a single sample (untrained)\")\n",
    "    :draw();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that's trivial in autograd is to add more arithmetic to the model,\n",
    "without having to worry about updating the update step (because that's taken care of for you\n",
    "by using automatic differentiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda = 1e-2\n",
    "function f(params, input, target)\n",
    "   prediction = predict_cnn(params, input)\n",
    "   loss = fCriterion(prediction, target)\n",
    "               + lambda*torch.sum(torch.pow(params[1],2)) -- add in some regularization\n",
    "               + lambda*torch.sum(torch.pow(params[3],2))\n",
    "               + lambda*torch.sum(torch.pow(params[5],2))\n",
    "\n",
    "   return loss, prediction\n",
    "end\n",
    "\n",
    "-- Autograd will give you a function that calculates gradients\n",
    "df = grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Try training with regularization, or make other fun modifications!  \n",
    "\n",
    "Note that it's not too crazy to chain together multiple networks using autograd. Use it as \"neural network glue\"! For instance, for video classification, try merging a CNN with an LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals achieved:**\n",
    "* Understand torch and the neural networks package at a high-level.\n",
    "* Train a small neural network on CPU and GPU\n",
    "* Use autograd to take gradients of more complicated functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What else should I try next?**:\n",
    "\n",
    "* Dig in more to autograd (and join the Slack channel): https://github.com/twitter/torch-autograd\n",
    "* Train on imagenet with multiple GPUs: https://github.com/soumith/imagenet-multiGPU.torch\n",
    "* Train recurrent networks with LSTM on text: https://github.com/wojzaremba/lstm\n",
    "* More demos and tutorials: https://github.com/torch/torch7/wiki/Cheatsheet\n",
    "* Chat with developers of Torch: http://gitter.im/torch/torch7\n",
    "* Ask for help: http://groups.google.com/forum/#!forum/torch7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
